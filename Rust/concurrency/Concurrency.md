## 2x2 Concurrency Matrix

To capture the interplay between the two most important pivots (Task Nature and Memory Model), we can use a 2x2 matrix. This shows how different tools and patterns fit into each quadrant.

| .                                | **Shared State**  <br>(Communicate by sharing memory)                                                                                                                                                                                                                                                | **Message Passing**  <br>(Share memory by communicating)                                                                                                                                                                                                                                                                   |
| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **CPU-Bound**  <br>(Parallelism) | **Quadrant 1: Parallel Computation**  <br>Multiple threads work on the same complex data structure.  <br>**Tools:** `std::thread` + `Arc<Mutex<T>>` for complex data, `rayon` with `Mutex`es. **`Arc<AtomicUsize>`**Â (for simple counters/flags),<br> **Use Case:** In-memory scientific simulation. | **Quadrant 2: Data Flow Pipelines**  <br>Raw data is fed to a pool of worker threads; results are collected.  <br>**Tools:** `std::thread` + `std::sync::mpsc::channel`. <br> **Use Case:** A multi-threaded video encoder.                                                                                                |
| **I/O-Bound**  <br>(Async)       | **Quadrant 3: Cached/Shared Resources**  <br>Many async tasks need access to a shared resource, like a connection pool or in-memory cache.  <br>**Tools:** `tokio::spawn` + `Arc<Mutex<T>>`. <br> **Use Case:** A web server with a shared state object.                                             | **Quadrant 4: Service-Oriented / Actor Systems**  <br>Independent services or actors handle requests without sharing state. The "purest" async model.  <br>**Tools:** `tokio::spawn` + `tokio::sync::channel`, Actor frameworks like `actix`. <br> **Use Case:** A microservices architecture, a high-traffic chat server. |
This matrix provides a much more nuanced view. It shows that `Mutex` isn't just for threads, and channels aren't just for async. It forces you to think about both the **nature of the work** and the **architecture of your data flow** to choose the right combination of tools for the job.
### The Concurrency Toolbox

#### Locks (For Shared State)

- **`std::sync::Mutex<T>`**
    - **What:** A standard, blocking mutual exclusion lock.
    - **Use Case:** The general-purpose lock for protecting data in multi-threaded, CPU-bound contexts. The thread will sleep until the lock is available.
- **`std::sync::RwLock<T>`**
    - **What:** A standard, blocking Read-Write lock.
    - **Use Case:** For the "Many Readers" problem in CPU-bound contexts. Allows unlimited concurrent readers OR one exclusive writer.
- **std::sync::atomicÂ Module (AtomicUsize,Â AtomicBool, etc.)**
    - **What:**Â Primitive types that can be safely modified across threads without a mutex. Operations are performed via indivisible hardware instructions.
    - **Use Case:**Â High-performance, low-contention scenarios for simple data. Perfect for shared counters, flags, or sequence numbers. Use when the overhead of aÂ MutexÂ is too high for a very simple, frequent operation. Avoid for complex logic, which can lead to inefficient spin-locking.
- **`tokio::sync::Mutex<T>`**
    - **What:** An async-aware mutex.
    - **Use Case:** Must be used in async code when a lock might be held across an `.await` point. It yields control back to the async runtime instead of blocking the OS thread.
- **`tokio::sync::RwLock<T>`**
    - **What:** An async-aware Read-Write lock.
    - **Use Case:** The async solution to the "Many Readers" problem. Ideal for a shared cache in a web server.
---
#### Channels (For Message Passing)
- **`std::sync::mpsc::channel`**
    - **What:** A standard, blocking, Multi-Producer, **Single-Consumer** (MPSC) channel.
    - **Use Case:** The basic channel included with Rust. Good for simple cases where one thread collects all results.
- **`crossbeam_channel::unbounded` or `::bounded`**
    - **What:** A high-performance, blocking, **Multi-Producer, Multi-Consumer** (MPMC) channel.
    - **Use Case:** A powerful replacement for `std::sync::mpsc`. Use this in CPU-bound contexts when you need multiple workers to both send and receive from the same channel, or when you need higher performance.
- **`tokio::sync::mpsc::channel`**
    - **What:** An async, bounded, Multi-Producer, Single-Consumer (MPSC) channel.
    - **Use Case:** A work queue for a specific async task that processes items one by one.
- **`tokio::sync::broadcast::channel`**
    - **What:** An async, bounded, Multi-Producer, Multi-Consumer (MPMC) channel where every consumer sees every message.
    - **Use Case:** Broadcasting events to all interested listeners, like a shutdown signal or live state updates.
- **`tokio::sync::oneshot::channel`**
    - **What:** An async, Single-Producer, Single-Consumer channel for sending a **single value**.
    - **Use Case:** The perfect tool for getting a result back from a spawned task. Essential for the "CPU Offload" pattern.
---
### The Decision-Making Concurrency Questionnaire ðŸ¤”

#### Step 1: What is the nature of your task?
- **Question:** Is your code's performance limited by **CPU calculations** or by waiting for **external I/O**?
    - **A) CPU-Bound** â†’ You need **Parallelism**. Your path starts with `std::thread`. Proceed to Step 2.
    - **B) I/O-Bound** â†’ You need **Asynchronous Concurrency**. Your path starts with `tokio::spawn`. Proceed to Step 2.
---
#### Step 2: How will your tasks access data?
- **Question:** Do your tasks need to operate on one large, central piece of data, or can the work be broken down into independent, self-contained chunks?
    - **A) Central Data** â†’ Your approach is **Shared State**. Proceed to Step 3.
    - **B) Independent Chunks** â†’ Your approach is **Message Passing**. Proceed to Step 3.
---
#### Step 3: How will your tasks interact?
- _(If you chose Shared State in Step 2)_
	- **Question:**Â What kind of data are you protecting?
	    - **A) A simple primitive type (u64,Â bool, etc.) for simple operations (incrementing, setting a flag)?**Â â†’ Your best tool is anÂ **Atomic**Â type (e.g.,Â std::sync::atomic::AtomicUsize).
	    - **B) A complex data structure (Vec,Â HashMap, a customÂ struct)?**Â â†’ Proceed to the next question.
	        - **Question:**Â Is this shared data read much more often than it is written to?
	            - **Yes (Read-Mostly)**Â â†’ Your primary tool is aÂ **RwLock**.
	            - **No (Frequent Writes)**Â â†’ Your primary tool is aÂ **Mutex**.
- _(If you chose Message Passing in Step 2)_
    - **Question:** What is your communication pattern?
        - **A) Distributing many jobs to one or more workers?** â†’ Use an **`mpsc`** or **`crossbeam`** channel.
        - **B) Getting a single result back from one task?** â†’ Use a **`oneshot`** channel.
        - **C) Sending the same event to all listeners?** â†’ Use a **`broadcast`** channel.
---
### Final Recommendation âœ…
Based on your answers, here is your recommended starting point:
- **1A + 2A (CPU-Bound + Shared State) â†’ Quadrant 1**
    - **Tools:** `std::thread` combined with...
	    - `Arc<std::sync::Mutex<T>>Â orÂ Arc<std::sync::RwLock<T>>`Â for complex data.
		- `Arc<std::sync::atomic::AtomicUsize>`Â for simple, high-performance counters or flags.
- **1A + 2B (CPU-Bound + Message Passing) â†’ Quadrant 2**
    - **Tools:** `std::thread` combined with **`crossbeam_channel`** (for MPMC) or **`std::sync::mpsc`** (for MPSC).
- **1B + 2A (I/O-Bound + Shared State) â†’ Quadrant 3**
    - **Tools:** `tokio::spawn` combined with `Arc<tokio::sync::Mutex<T>>` or `Arc<tokio::sync::RwLock<T>>`.
- **1B + 2B (I/O-Bound + Message Passing) â†’ Quadrant 4**
    - **Tools:** `tokio::spawn` combined with `tokio::sync` channels (`mpsc`, `oneshot`, `broadcast`).

## Sample codes

### **Notes: CPU-Bound Quadrants**

This section covers parallel processing for computationally intensive tasks that are limited by CPU speed, not by waiting for external resources like networks or disks.

#### **Core Philosophy: The Two Models**
1. **Quadrant 2: Message Passing ("Share Memory by Communicating")**
    - **Concept:**Â Threads are isolated and do not touch the same data directly. They transfer ownership of data by sending messages over channels. This is often easier to reason about and less prone to deadlocks.
    - **Analogy:**Â A team of people sending emails. Once you send the email (the data), you don't edit it anymore; it belongs to the receiver.
2. **Quadrant 1: Shared State ("Communicate by Sharing Memory")**
    - **Concept:**Â Multiple threads have access to the same piece of memory. To prevent chaos and data races, access is controlled via synchronization primitives likeÂ Mutexes orÂ Atomics.
    - **Analogy:**Â A team of people editing the same document on a shared whiteboard. Only one person can write at a time (holding the marker/lock).
### **Quadrant 1 - Shared State in Practice**
- **Primary Tools:**
    - `std::sync::Mutex<T>`: A "sleeping" lock that uses the OS to ensure mutual exclusion. It's the general-purpose tool for protecting any complex dataÂ T.
    - `std::sync::atomic`: "Lock-free" primitives (AtomicU32,Â AtomicBool, etc.) that use special CPU instructions for unparalleled performance on simple types.
    - `std::sync::Arc<T>`: Atomic Reference Counting. Essential for giving multiple threads shared ownership of aÂ MutexÂ orÂ Atomic.
- **Key Challenge & Solution (Shared Counter):**
    - **Problem:**Â Have 10 threads each increment a counter 100 times. Final result must be 1000.
    - **MutexÂ Solution:**
        1. Create state:Â let counter = Arc::new(Mutex::new(0));
        2. In each thread, loop 100 times.
        3. Inside the loop, the "critical section" is:
		    ```rust
            let mut guard = counter.lock().unwrap(); // Acquire lock
            *guard += 1;                           // Mutate data
            // `guard` is dropped here, automatically releasing the lock
            ```
    - **AtomicÂ Solution (Better for this case):**
        1. Create state:Â let counter = Arc::new(AtomicU32::new(0));
        2. In each thread, call the atomic operation:
            ```rust
            counter.fetch_add(1, Ordering::SeqCst);
            ```
        3. Read the final result withÂ counter.load(Ordering::SeqCst).
- **My Questions & The Answers:**
    1. **Why doesÂ MutexÂ have "overhead"?**
        - It involvesÂ **system calls**Â to the OS kernel to manage the lock state and put waiting threads to sleep. This process (context switching) is expensive compared to a simple CPU instruction.
    2. **What does "lock-free" (Atomic) mean?**
        - It means free of OS-level locks. The operation is guaranteed to be indivisible ("atomic") by aÂ **special CPU hardware instruction**. It's much faster but only works for primitive types that fit in a CPU word (likeÂ u32,Â bool, pointers). Threads don't "sleep"; they may "spin" (busy-wait) for a nanosecond if there's contention, which is handled by the hardware memory bus.
    3. **Why no genericÂ `Atomic<T>`?**
        - Because CPUs don't have instructions to atomically modify arbitrary, complex data structures (T). They only have instructions for simple, word-sized primitives.Â MutexÂ solves this by not touching the data, but rather guarding theÂ gateÂ to the data.
    4. **CanÂ MutexÂ be used withoutÂ Arc?**
        - Yes.Â ArcÂ is forÂ shared ownershipÂ across threads that may outlive the parent function. If you can guarantee theÂ MutexÂ will live long enough (e.g., usingÂ std::thread::scope), you can just borrow it (`&Mutex<T>`) instead.
    5. **Why do compilers/CPUs reorder instructions?**
        - ForÂ **performance**. They hide the latency of slow operations (like memory access) by executing other independent instructions in the meantime. MemoryÂ OrderingÂ is our way to tell them when this reordering is not safe because another thread depends on the order.Â SeqCstÂ is the strictest, safest "don't reorder" rule.
```rust
use std::thread;
use std::sync::{Arc, Mutex, MutexGuard};

fn main() {
    let counter: Arc<Mutex<u32>> = Arc::new(Mutex::new(0));
    let mut join_handles: Vec<thread::JoinHandle<()>> = vec![];
    
    for _ in 0..10 {
        let counter_clone = Arc::clone(&counter);
        let handle = thread::spawn(move || {
            for _ in 0..100 {
                let mut n: MutexGuard<u32> = counter_clone.lock().unwrap();
                *n += 1;
            }
        });
        join_handles.push(handle);
    }
    
    for handle in join_handles {
        let _ = handle.join();
    }
    
    println!("{}", counter.lock().unwrap()); // 1000
}

//// Atomics for primitives types sharing, locks have overhead
use std::thread;
use std::sync::{Arc};
use std::sync::atomic::{AtomicU32, Ordering};

fn main() {
    let counter: Arc<AtomicU32> = Arc::new(AtomicU32::new(0));
    let mut join_handles: Vec<thread::JoinHandle<()>> = vec![];
    
    for _ in 0..10 {
        let counter_clone = Arc::clone(&counter);
        let handle = thread::spawn(move || {
            for _ in 0..100 {
                counter_clone.fetch_add(1, Ordering::SeqCst);
            }
        });
        join_handles.push(handle);
    }
    
    for handle in join_handles {
        let _ = handle.join();
    }
    
    println!("{}", counter.load(Ordering::SeqCst)); // 1000
}
```

### **Quadrant 2 - Message Passing in Practice**
- **Primary Tools:**
    - `std::thread::spawn`: Creates a new OS thread. The closure passed to itÂ **must**Â beÂ moveÂ to take ownership of its environment.
    - `std::sync::mpsc::channel`: Creates aÂ **M**ultipleÂ **P**roducer,Â **S**ingleÂ **C**onsumer channel. Returns aÂ (Sender, Receiver)Â tuple.
	    - `let (sender, receiver) = channel();`
	    - General terminology `let (tx, rx) = channel();` - `tx - Transmission`, `rx - Receive`.
- **Key Challenge & Solution (Worker Pool Pattern):**
    - **Problem:**Â AnÂ mpscÂ channel has only oneÂ Receiver. How can multiple worker threads all receive jobs from the same source?
    - **Solution:**Â The singleÂ ReceiverÂ must be shared. We wrap it inÂ `Arc<Mutex<T>>`.
        - Arc: Allows multiple threads to have a shared ownership handle to theÂ Mutex.
        - Mutex: Ensures only one worker thread can `callÂ .recv()`Â at a time.
    - **My Implementation:**
        1. Main thread createsÂ job_channelÂ andÂ result_channel.
        2. job_rxÂ is wrapped:Â `let job_rx = Arc::new(Mutex::new(job_rx));`
        3. Spawn workers, giving each aÂ clone()Â of theÂ `Arc<Mutex<job_rx>>`Â and aÂ clone()Â of theÂ result_tx.
        4. **Graceful Shutdown:**Â Main thread sends all jobs, then callsÂ `drop(job_tx)`. This closes the channel.
        5. Worker threadsÂ loop, callingÂ `job_rx.lock().unwrap().recv()`. When it returnsÂ Err, it means the channel is closed, and the thread breaks its loop and terminates.
- **Advanced Insight (Performance): Minimize Lock Contention**
    - **Mistake:**Â Holding aÂ MutexÂ lock for longer than necessary. In my first implementation, the lock was held while the worker was "processing" the job (thread::sleep).
    - **Optimization:**Â Hold the lockÂ onlyÂ to perform the atomic action (.recv()). Release it immediately, then do the heavy processing.
```rust
use std::thread;
use std::sync::mpsc::channel;
use std::sync::{Arc, Mutex};
use std::time::Duration;

const NUM_WORKERS: u32 = 4;
const NUM_JOBS: u32 = 20;

fn main() {
    let (job_send, job_rcv) = channel::<u32>();
    let (result_send, result_rcv) = channel::<u32>();
    let job_rcv = Arc::new(Mutex::new(job_rcv));
    let mut job_handles: Vec<thread::JoinHandle<()>> = vec![];
    
    // Workers receiving job & sending the result
    for i in 1..=NUM_WORKERS {
        let job_rcv_clone = Arc::clone(&job_rcv);
        let result_send_clone = result_send.clone();
        let join_handle = thread::spawn(move || {
            loop {
                let job_rcv = {
                    job_rcv_clone.lock().unwrap().recv()
                }; // Lock & get data & unlock immediately
                match job_rcv {
                    Ok(n) => {
                        println!("Worker {i} got job {n}");
                        thread::sleep(Duration::from_millis(50));
                        let _ = result_send_clone.send(n * 2);
                    },
                    Err(..) => break,
                }
            }
        });
        job_handles.push(join_handle);
    }
    
    // Main thread sending the job info
    for i in 1..=NUM_JOBS {
        let _ = job_send.send(i);
    }
    // Sending the Err, to notify the jobs are done.
    drop(job_send);
    
    // Main thread receving the result
    for _ in 1..=NUM_JOBS {
        println!("\t{:?}", result_rcv.recv());
    }
    
    // Wait for all threads to complete.
    for handle in job_handles {
        let _ = handle.join();
    }
}
```

### **I/O-Bound Quadrants (Async Rust)**
This section covers concurrency for tasks that are limited by waiting for external operations (I/O), such as network requests, file system access, or database queries. The goal is to handle many concurrent waiting tasks efficiently without wasting threads.
#### **Core Philosophy: Async - "Waiting Without Wasting"**
- **Concept:**Â Instead of blocking an entire OS thread while waiting, anÂ asyncÂ task yields control back to a scheduler. This scheduler, called aÂ **runtime**Â (e.g.,Â tokio), can then use the now-free thread to run other tasks that are ready to do work. 
- **Key Primitives:**
    - async fn: A function that returns aÂ FutureÂ (a state machine representing a value that may not be ready yet).Â Futures are lazy and do nothing until polled.
    - .await: The operator used inside anÂ async fnÂ to pause the current task and wait for anotherÂ FutureÂ to complete, allowing the runtime to execute other tasks.
    - **Runtime (tokio)**: The engine that pollsÂ Futures, manages waking them up, and schedules them on a small pool of worker threads. YourÂ mainÂ function must be designated with an attribute likeÂ `#[tokio::main]`.
- **Critical Insight: Spawning for Concurrency**
    - Simply callingÂ .awaitÂ on multiple futures in a row results inÂ **sequential**, not concurrent, execution.
    - To run futures concurrently, they must be handed off to the runtime's scheduler using a mechanism likeÂ tokio::spawn. This creates a new, independent task that the runtime manages in the background.
### **Quadrant 4 - Async with Message Passing**

- **Concept:**Â Tasks are designed to be highly independent, like miniature services or actors. They don't share state; they communicate by receiving data to begin their work and returning data when they are done.
- **"Message Passing" Styles:**
    1. **Implicit/Conceptual:**Â The function call itself is the "in" message (the arguments), and theÂ returnÂ value is the "out" message. This is the most common style.
    2. **Explicit:**Â UsingÂ `tokio::sync::channel`Â to create an async-aware channel. This is used for building more formal actor systems where a long-lived task processes a stream of incoming message-enums.
- **Key Challenge & Solution (Managing Concurrent Tasks):**
    - **Problem:**Â How to launch many concurrent downloads and gather their results.
    - **Solution:Â tokio::task::JoinSet**: This is the modern, idiomatic tool for this pattern.
        1. Create aÂ JoinSet.
        2. UseÂ `set.spawn(my_async_fn())`Â to add tasks to the set.
        3. Use aÂ while `let Some(result) = set.join_next().await`Â loop to process resultsÂ **as they complete**. This is more responsive than waiting for all tasks to finish.
    - **My Implementation:**Â Created aÂ download(url)Â async function. Spawned one for each URL into aÂ JoinSet. TheÂ join_nextÂ loop then processed the content of each download as soon as it became available.
- **Critical Insight: TheÂ JoinHandleÂ "Leash"**
    - AÂ tokioÂ runtime will shut down when its main task completes. If youÂ spawnÂ tasks but don'tÂ .awaitÂ theirÂ JoinHandles (either directly or via aÂ JoinSet), theÂ mainÂ task will finish, and the runtime will shut down, likely aborting your spawned tasks before they can complete. Awaiting a handle creates a dependency that keeps the runtime alive.
```rust
use std::time::Duration;
use tokio::task;
use tokio::time;

async fn download(url: &str) -> String {
    println!("Starting download for {url}");
    let sleep_duration = rand::random_range(1..=5);
    time::sleep(Duration::from_secs(sleep_duration)).await;
    println!("Finished download for {url}. Took {sleep_duration} sec(s).");
    format!("{url} content")
}

#[tokio::main]
async fn main() {
    let urls = vec!["url1", "url2", "url3"];
    let mut set = task::JoinSet::<String>::new();
    
    for url in urls {
        set.spawn(async move {
            download(url).await
        });
    }
    
    // let output = set.join_all().await; // Waits for all to complete
    while let Some(result) = set.join_next().await { // Prints results as they complete 
        match result {
            // The task completed successfully.
            Ok(content) => {
                println!("Processed result: '{}'", content);
            }
            // The task failed.
            Err(e) => {
                // You can check if the task panicked
                if e.is_panic() {
                    println!("A task panicked!");
                } else {
                    println!("A task failed to complete: {}", e);
                }
            }
        }
    }
    println!("\n--- All tasks complete ---");
}
```
### **Quadrant 3 - Async with Shared State**
- **Concept:**Â Multiple async tasks need to access the same shared data (e.g., a connection pool, in-memory cache). This is the intersection of theÂ asyncÂ world and theÂ `Arc<Mutex<T>>`Â world.
- **The Critical Rule: Use the RightÂ Mutex**
    - **NEVER**Â `useÂ std::sync::Mutex`Â if the lock will be held across anÂ .awaitÂ point.
    - **Why?**Â A standardÂ MutexÂ blocks theÂ OS thread. If an async task holds this lock and thenÂ .awaits, the thread is put to sleep by the OS and is no longer available to the async runtime. This can starve the runtime of worker threads and easily lead to a complete deadlock.
    - **Solution:Â tokio::sync::Mutex**: This is an async-aware mutex. When itsÂ .lock().awaitÂ method is called on a contested lock, it doesÂ **not**Â block the thread. Instead, it cooperates with the async scheduler, suspending theÂ taskÂ and allowing the thread to be used for other work.
- **Key Challenge & Solution (Async Shared Counter):**
    - **Problem:**Â Simulate many concurrent web requests incrementing a shared page-view counter.
    - **Solution:**Â Combine the patterns.
        1. The state isÂ `Arc<tokio::sync::Mutex<u32>>`.
        2. Create anÂ async fn handle_requestÂ that takes a clone of theÂ Arc.
        3. Inside the function, the critical section is:
            ```rust
            let mut num = counter.lock().await; // Async lock acquisition
            *num += 1;
            // Async lock is released when `num` is dropped
            ```
        4. InÂ main, use aÂ JoinSetÂ to spawn 100Â handle_requestÂ tasks, then wait for them all to complete.
        5. The final result is correct because theÂ tokio::sync::MutexÂ ensures that theÂ *num += 1Â operation is atomic with respect to other tasks.
```rust
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::Mutex;
use tokio::task;
use tokio::time;

async fn handle_request(counter: Arc<tokio::sync::Mutex<u32>>) {
    time::sleep(Duration::from_secs(1)).await;
    let mut num = counter.lock().await;
    *num += 1;
}

#[tokio::main]
async fn main() {
    let counter: Arc<Mutex<u32>> = Arc::new(Mutex::new(0));
    let mut set = task::JoinSet::<()>::new();
    
    for _ in 0..100 {
        let counter_clone = Arc::clone(&counter);
        set.spawn(async move {
            handle_request(counter_clone).await
        });
    }
    
    let _ = set.join_all().await;
    
    println!("Final result {}", counter.lock().await);
}
```

## Rayon
`rayon`Â is a data-parallelism library. Its philosophy is: "If your code works on an iterator, it can be made parallel with a one-line change." It's designed to be a nearly drop-in replacement forÂ `std::iter::Iterator`.

It achieves this through a cleverÂ **work-stealing**Â thread pool, much like Tokio's, but optimized for CPU-bound tasks. When you convert an iterator into a parallel iterator,Â rayonÂ splits the work into smaller and smaller chunks until it has enough pieces to keep all your CPU cores busy. If one core finishes its work early, it "steals" a chunk of work from another, busier core.

**The Key Trait:**Â `rayon::iter::ParallelIterator`.  
**The Key Method:**Â `.par_iter()Â (orÂ .par_iter_mut(),Â .into_par_iter())`. You call this on a collection to turn it into a parallel iterator. From there, you can use most of the same adaptors you already know:Â `map,Â filter,Â fold,Â reduce,Â collect,` etc.

However, some iterator methods areÂ **inherently sequential**Â and cannot be meaningfully parallelized.
- The most obvious example isÂ **.next()**. A standard iterator produces one item at a time. A parallel iterator operates on chunks of data at once; there is no single "next" item to produce.
- Other sequential methods likeÂ **.scan()**Â (which relies on state from the previous iteration) orÂ **.try_fold()**Â (which must process elements in order to short-circuit) don't have direct parallel equivalents because their logic is fundamentally sequential.

**The Rule of Thumb:**Â If an operation can be broken down into independent chunks of work that can be combined later,Â rayonÂ almost certainly has a parallel version of it. If the operation relies on a specific, strict, element-by-element order, it probably doesn't.

```rust
use rayon::prelude::*;
use std::time::Instant;

const N: u32 = 2_000_000;

pub fn is_prime(n: u32) -> bool {
    if n < 2 { return false; }
    for i in 2..=n.isqrt() {
        if n % i == 0 { return false; }
    }
    true
}

fn main() {
    let now = Instant::now();

    let primes = (2..=N)
        .into_iter()
        .filter(|i| is_prime(*i))
        .count();
        
    println!("{} ms, len: {}", now.elapsed().as_millis(), primes);
    
    let now = Instant::now();

    let primes = (2..=N)
        .into_par_iter()
        .filter(|i| is_prime(*i))
        .count();
        
    println!("{} ms, len: {}", now.elapsed().as_millis(), primes);
}
```